{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sentencepiece as spm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(os.getcwd())\n",
    "mainpath = cwd.parents[0] / \"data/raw\"\n",
    "languagepaths = [x for x in mainpath.iterdir() if x.is_dir()]\n",
    "languages = [x.name for x in languagepaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english News Dev\n",
      "english News Test\n",
      "english News Train\n",
      "english WikiNews Dev\n",
      "english WikiNews Test\n",
      "english WikiNews Train\n",
      "english Wikipedia Dev\n",
      "english Wikipedia Test\n",
      "english Wikipedia Train\n",
      "french French Test\n",
      "german German Dev\n",
      "german German Test\n",
      "german German Train\n",
      "spanish Spanish Dev\n",
      "spanish Spanish Test\n",
      "spanish Spanish Train\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.DataFrame()\n",
    "\n",
    "fieldnames = ['hit_id', 'sentence', 'start_offset', 'end_offset', 'target_word', 'native_annots', \n",
    "              'nonnative_annots', 'native_complex', 'nonnative_complex', 'gold_label', 'gold_prob']\n",
    "\n",
    "relevant_cols = ['sentence', 'target_word', 'gold_label']\n",
    "\n",
    "for lang in languagepaths:\n",
    "    for datasource in lang.iterdir():\n",
    "        source_data_type = datasource.stem.split('_')\n",
    "        source = source_data_type[0]\n",
    "        data_type = source_data_type[1]\n",
    "        print(lang.name, source, data_type)\n",
    "        source_df = pd.read_csv(datasource, sep='\\t', header=None, names=fieldnames)\n",
    "        relevant_df = source_df[relevant_cols]\n",
    "        \n",
    "        relevant_df.is_copy = False\n",
    "        \n",
    "        relevant_df['lang'] = lang.name\n",
    "        relevant_df['source'] = source\n",
    "        relevant_df['data_type'] = data_type\n",
    "        \n",
    "        \n",
    "        full_df = full_df.append(relevant_df, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_word</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45035</th>\n",
       "      <td>Los Bronces de Riace conocidos también como Lo...</td>\n",
       "      <td>Los Bronces de Riace</td>\n",
       "      <td>1</td>\n",
       "      <td>spanish</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45036</th>\n",
       "      <td>Los Bronces de Riace conocidos también como Lo...</td>\n",
       "      <td>Bronces</td>\n",
       "      <td>1</td>\n",
       "      <td>spanish</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45037</th>\n",
       "      <td>Los Bronces de Riace conocidos también como Lo...</td>\n",
       "      <td>Riace</td>\n",
       "      <td>1</td>\n",
       "      <td>spanish</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45038</th>\n",
       "      <td>Los Bronces de Riace conocidos también como Lo...</td>\n",
       "      <td>griegas</td>\n",
       "      <td>1</td>\n",
       "      <td>spanish</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45039</th>\n",
       "      <td>Los Bronces de Riace conocidos también como Lo...</td>\n",
       "      <td>conocidos</td>\n",
       "      <td>0</td>\n",
       "      <td>spanish</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "45035  Los Bronces de Riace conocidos también como Lo...   \n",
       "45036  Los Bronces de Riace conocidos también como Lo...   \n",
       "45037  Los Bronces de Riace conocidos también como Lo...   \n",
       "45038  Los Bronces de Riace conocidos también como Lo...   \n",
       "45039  Los Bronces de Riace conocidos también como Lo...   \n",
       "\n",
       "                target_word  gold_label     lang   source data_type  \n",
       "45035  Los Bronces de Riace           1  spanish  Spanish       Dev  \n",
       "45036               Bronces           1  spanish  Spanish       Dev  \n",
       "45037                 Riace           1  spanish  Spanish       Dev  \n",
       "45038               griegas           1  spanish  Spanish       Dev  \n",
       "45039             conocidos           0  spanish  Spanish       Dev  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.loc[full_df['lang'] == 'spanish'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_langs = ['spanish', 'english', 'german','french']\n",
    "# train_langs = ['spanish', 'english', 'german']\n",
    "\n",
    "test_langs = ['english']\n",
    "train_langs = ['english']\n",
    "\n",
    "all_langs = set(test_langs + train_langs)\n",
    "\n",
    "test_data = full_df.loc[(full_df['lang'].isin(test_langs)) & (full_df['data_type'] == 'Test' )]\n",
    "dev_data = full_df.loc[(full_df['lang'].isin(test_langs)) & (full_df['data_type'] == 'Dev' )]\n",
    "\n",
    "train_data = full_df.loc[(full_df['lang'].isin(train_langs)) & (full_df['data_type'] == 'Train' )]\n",
    "\n",
    "# 50% split on French data as dev, 100% still to test\n",
    "# Next week write about monolingual model\n",
    "# Frequency of the target word in learner corpus\n",
    "\n",
    "target = pd.concat([test_data, dev_data, train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_len_words(sentence):\n",
    "    return len(sentence.split())\n",
    "\n",
    "def get_sent_len_chars(sentence):\n",
    "    return len(sentence)\n",
    "\n",
    "def get_sent_len_chars_avg(sentence):\n",
    "    return len(sentence)/len(sentence.split())\n",
    "\n",
    "def get_num_target_words(words):\n",
    "    result = len(words.split(' '))\n",
    "    return result\n",
    "\n",
    "def get_avg_word_len(words):\n",
    "    num_words = len(words.split(' '))\n",
    "    total_len_char = len(words)\n",
    "    \n",
    "    # Removing the spaces\n",
    "    num_spaces = num_words - 1\n",
    "    word_chars = total_len_char - num_spaces\n",
    "    \n",
    "    result = word_chars/num_words\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_word</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>data_type</th>\n",
       "      <th>sent_len_w</th>\n",
       "      <th>sent_len_c</th>\n",
       "      <th>sent_len_c_avg</th>\n",
       "      <th>avg_target_len</th>\n",
       "      <th>num_target_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>The teenage girl shot dead in Bellaghy, County...</td>\n",
       "      <td>teenage</td>\n",
       "      <td>0</td>\n",
       "      <td>english</td>\n",
       "      <td>News</td>\n",
       "      <td>Test</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>6.9375</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>The teenage girl shot dead in Bellaghy, County...</td>\n",
       "      <td>teenage girl</td>\n",
       "      <td>0</td>\n",
       "      <td>english</td>\n",
       "      <td>News</td>\n",
       "      <td>Test</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>6.9375</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>The teenage girl shot dead in Bellaghy, County...</td>\n",
       "      <td>Londonderry</td>\n",
       "      <td>1</td>\n",
       "      <td>english</td>\n",
       "      <td>News</td>\n",
       "      <td>Test</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>6.9375</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>The teenage girl shot dead in Bellaghy, County...</td>\n",
       "      <td>girl</td>\n",
       "      <td>0</td>\n",
       "      <td>english</td>\n",
       "      <td>News</td>\n",
       "      <td>Test</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>6.9375</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>The teenage girl shot dead in Bellaghy, County...</td>\n",
       "      <td>shot</td>\n",
       "      <td>0</td>\n",
       "      <td>english</td>\n",
       "      <td>News</td>\n",
       "      <td>Test</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>6.9375</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence   target_word  \\\n",
       "1764  The teenage girl shot dead in Bellaghy, County...       teenage   \n",
       "1765  The teenage girl shot dead in Bellaghy, County...  teenage girl   \n",
       "1766  The teenage girl shot dead in Bellaghy, County...   Londonderry   \n",
       "1767  The teenage girl shot dead in Bellaghy, County...          girl   \n",
       "1768  The teenage girl shot dead in Bellaghy, County...          shot   \n",
       "\n",
       "      gold_label     lang source data_type  sent_len_w  sent_len_c  \\\n",
       "1764           0  english   News      Test          16         111   \n",
       "1765           0  english   News      Test          16         111   \n",
       "1766           1  english   News      Test          16         111   \n",
       "1767           0  english   News      Test          16         111   \n",
       "1768           0  english   News      Test          16         111   \n",
       "\n",
       "      sent_len_c_avg  avg_target_len  num_target_w  \n",
       "1764          6.9375             7.0             1  \n",
       "1765          6.9375             5.5             2  \n",
       "1766          6.9375            11.0             1  \n",
       "1767          6.9375             4.0             1  \n",
       "1768          6.9375             4.0             1  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.is_copy = False\n",
    "target['sent_len_w'] = target.apply(func= lambda row : get_sent_len_words( row['sentence'] ) , axis=1)\n",
    "target['sent_len_c'] = target.apply(func= lambda row : get_sent_len_chars( row['sentence'] ) , axis=1)\n",
    "target['sent_len_c_avg'] = target.apply(func= lambda row : get_sent_len_chars_avg( row['sentence'] ) , axis=1)\n",
    "target['avg_target_len'] = target.apply(func= lambda row : get_avg_word_len( row['target_word'] ) , axis=1)\n",
    "target['num_target_w'] = target.apply(func= lambda row :  get_num_target_words( row['target_word'] ) , axis=1)\n",
    "\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sent_len_w</th>\n",
       "      <th>sent_len_c</th>\n",
       "      <th>sent_len_c_avg</th>\n",
       "      <th>avg_target_len</th>\n",
       "      <th>num_target_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34879.000000</td>\n",
       "      <td>34879.000000</td>\n",
       "      <td>34879.000000</td>\n",
       "      <td>34879.000000</td>\n",
       "      <td>34879.000000</td>\n",
       "      <td>34879.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.413659</td>\n",
       "      <td>28.639325</td>\n",
       "      <td>170.933083</td>\n",
       "      <td>5.994051</td>\n",
       "      <td>6.721277</td>\n",
       "      <td>1.199633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.492496</td>\n",
       "      <td>13.063224</td>\n",
       "      <td>78.405141</td>\n",
       "      <td>0.630687</td>\n",
       "      <td>2.285897</td>\n",
       "      <td>0.593245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.947368</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>5.578947</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>5.982143</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>6.393939</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>657.000000</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         gold_label    sent_len_w    sent_len_c  sent_len_c_avg  \\\n",
       "count  34879.000000  34879.000000  34879.000000    34879.000000   \n",
       "mean       0.413659     28.639325    170.933083        5.994051   \n",
       "std        0.492496     13.063224     78.405141        0.630687   \n",
       "min        0.000000      3.000000     16.000000        2.947368   \n",
       "25%        0.000000     20.000000    116.000000        5.578947   \n",
       "50%        0.000000     27.000000    158.000000        5.982143   \n",
       "75%        1.000000     35.000000    209.000000        6.393939   \n",
       "max        1.000000    110.000000    657.000000        8.333333   \n",
       "\n",
       "       avg_target_len  num_target_w  \n",
       "count    34879.000000  34879.000000  \n",
       "mean         6.721277      1.199633  \n",
       "std          2.285897      0.593245  \n",
       "min          2.000000      1.000000  \n",
       "25%          5.000000      1.000000  \n",
       "50%          6.500000      1.000000  \n",
       "75%          8.000000      1.000000  \n",
       "max         22.000000     11.000000  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = target.loc[target['data_type'] == 'Train']\n",
    "test = target.loc[target['data_type'] == 'Test']\n",
    "dev = target.loc[target['data_type'] == 'Dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_len_w\t -0.01439\n",
      "sent_len_c\t -0.00577\n",
      "sent_len_c_avg\t  0.05746\n",
      "avg_target_len\t  0.38357\n",
      "num_target_w\t  0.21694\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = target._get_numeric_data()\n",
    "\n",
    "# Printing simple correlations\n",
    "for colname in numeric_cols:\n",
    "    if colname != 'gold_label':\n",
    "        print(\"{}\\t{:9.5f}\".format(colname, target['gold_label'].corr(target[colname])))\n",
    "\n",
    "# target['gold_label'].corr(target['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Check this is correct. Is gold label of 1 Complex or Non-complex?\n",
    "class_labels = ['Complex', 'Non-Complex']\n",
    "\n",
    "train_data = train._get_numeric_data()\n",
    "dev_data = dev._get_numeric_data()\n",
    "test_data = test._get_numeric_data()\n",
    "\n",
    "# Getting the train_features\n",
    "train_features = train_data.drop('gold_label', axis=1)\n",
    "train_feature_names = train_features.columns\n",
    "\n",
    "train_ys = train_data['gold_label'].values\n",
    "train_Xs = train_features.values\n",
    "\n",
    "# Getting the dev_features\n",
    "dev_features = dev_data.drop('gold_label', axis=1)\n",
    "dev_feature_names = dev_features.columns\n",
    "\n",
    "dev_ys = dev_data['gold_label'].values\n",
    "dev_Xs = dev_features.values\n",
    "\n",
    "#Getting the test_features\n",
    "test_features = test_data.drop('gold_label', axis=1)\n",
    "test_feature_names = test_features.columns\n",
    "\n",
    "test_ys = test_data['gold_label'].values\n",
    "test_Xs = test_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Xs:\t(27299, 5)\n",
      "train_ys:\t(27299,)\n",
      "dev_Xs:\t\t(3328, 5)\n",
      "dev_ys:\t\t(3328,)\n",
      "test_Xs:\t(4252, 5)\n",
      "test_ys:\t(4252,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"train_Xs:\\t{}\\ntrain_ys:\\t{}\\ndev_Xs:\\t\\t{}\\ndev_ys:\\t\\t{}\\ntest_Xs:\\t{}\\ntest_ys:\\t{}\\n\".format(str(train_Xs.shape), str(train_ys.shape), str(dev_Xs.shape), str(dev_ys.shape), str(test_Xs.shape), str(test_ys.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the columns:\n",
    "\n",
    "# Can't do this because we can't propagate the normalization factor as far as I can tell, so we're doing it a simpler way.\n",
    "# train_Xs_norm = tf.keras.utils.normalize(train_Xs, axis=-1,order=2)\n",
    "# dev_Xs_norm = tf.keras.utils.normalize(dev_Xs, axis=-1,order=2)\n",
    "# test_Xs_norm = tf.keras.utils.normalize(test_Xs, axis=-1,order=2)\n",
    "\n",
    "max_vals = np.max(train_Xs, axis=0)\n",
    "min_vals = np.min(train_Xs, axis=0)\n",
    "norm_factor = 1 / (max_vals - min_vals)\n",
    "train_Xs_norm = norm_factor * (train_Xs - max_vals) + 1\n",
    "dev_Xs_norm = norm_factor * (dev_Xs - max_vals) + 1\n",
    "test_Xs_norm = norm_factor * (test_Xs - max_vals) + 1\n",
    "\n",
    "\n",
    "train_ys_cat = keras.utils.to_categorical(train_ys)\n",
    "\n",
    "if dev_ys.shape[0] != 0:\n",
    "    dev_ys_cat = keras.utils.to_categorical(dev_ys)\n",
    "    \n",
    "test_ys_cat = keras.utils.to_categorical(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(5,)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "\n",
    "# Obtained from here: https://www.kaggle.com/teasherm/keras-metric-for-f-score-tf-only\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, \"int32\")\n",
    "    y_pred = tf.cast(tf.round(y_pred), \"int32\") # implicit 0.5 threshold via tf.round\n",
    "    y_correct = y_true * y_pred\n",
    "    sum_true = tf.reduce_sum(y_true, axis=1)\n",
    "    sum_pred = tf.reduce_sum(y_pred, axis=1)\n",
    "    sum_correct = tf.reduce_sum(y_correct, axis=1)\n",
    "    precision = sum_correct / sum_pred\n",
    "    recall = sum_correct / sum_true\n",
    "    f_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    f_score = tf.where(tf.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
    "    return tf.reduce_mean(f_score)\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "27299/27299 [==============================] - 2s 90us/step - loss: 0.5610 - acc: 0.7182 - f1_score: 0.7182\n",
      "Epoch 2/5\n",
      "27299/27299 [==============================] - 2s 57us/step - loss: 0.5418 - acc: 0.7365 - f1_score: 0.7365\n",
      "Epoch 3/5\n",
      "27299/27299 [==============================] - 2s 55us/step - loss: 0.5407 - acc: 0.7337 - f1_score: 0.7337\n",
      "Epoch 4/5\n",
      "27299/27299 [==============================] - 1s 49us/step - loss: 0.5390 - acc: 0.7356 - f1_score: 0.7356\n",
      "Epoch 5/5\n",
      "27299/27299 [==============================] - 1s 49us/step - loss: 0.5397 - acc: 0.7353 - f1_score: 0.7353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x246acce3710>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_Xs_norm, train_ys_cat, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4252/4252 [==============================] - 0s 76us/step\n",
      "\n",
      "Trained on English.\n",
      "\n",
      "Tested on English.\n",
      "  (test sets)\n",
      "\n",
      "Test accuracy:\t0.732596425155593\n",
      "Test f1:\t0.7325964252116651\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1 = model.evaluate(test_Xs_norm, test_ys_cat)\n",
    "\n",
    "training_string = \"\"\n",
    "for lang_i in range(len(train_langs)):\n",
    "    lang = train_langs[lang_i]\n",
    "    \n",
    "    format_string = \"{}, \"\n",
    "    if lang_i == len(train_langs) - 2:\n",
    "        format_string = \"{} and \"\n",
    "    elif lang_i == len(train_langs) - 1:\n",
    "        format_string = \"{}.\"\n",
    "        \n",
    "    to_append = format_string.format(lang.capitalize())\n",
    "    training_string += to_append\n",
    "print(\"\\nTrained on {}\".format(training_string))\n",
    "\n",
    "testing_string = \"\"\n",
    "for lang_i in range(len(test_langs)):\n",
    "    lang = test_langs[lang_i]\n",
    "    \n",
    "    format_string = \"{}, \"\n",
    "    if lang_i == len(test_langs) - 2:\n",
    "        format_string = \"{} and \"\n",
    "    elif lang_i == len(test_langs) - 1:\n",
    "        format_string = \"{}.\"\n",
    "        \n",
    "    to_append = format_string.format(lang.capitalize())\n",
    "    testing_string += to_append\n",
    "    \n",
    "print(\"\\nTested on {}\\n  (test sets)\".format(testing_string))\n",
    "\n",
    "print(\"\\nTest accuracy:\\t{}\\nTest f1:\\t{}\".format(test_acc, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytepairpath = cwd.parents[0] / \"data/external/bytepairencoding\"\n",
    "\n",
    "langs_to_langcodes = {'german':'de','english':'en','spanish':'es','french':'fr'}\n",
    "\n",
    "langs_to_bytepair_filepaths = {k: bytepairpath / (v + \".wiki.bpe.op10000.model\") for k,v in langs_to_langcodes.items()}\n",
    "langs_to_model_filepaths = {k: bytepairpath / (v + \".wiki.bpe.op10000.d300.w2v.bin\") for k,v in langs_to_langcodes.items()}\n",
    "\n",
    "langs_to_vocab_filepaths = {k: bytepairpath / (v + \".wiki.bpe.op10000.vocab.txt\") for k,v in langs_to_langcodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte pair encoding comes from here: https://github.com/bheinzerling/bpemb\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercase:\n",
    "    lowered = text.lower()\n",
    "    \n",
    "    non_latin = re.sub(r'[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]', \"\", lowered)\n",
    "    \n",
    "    # replace digits with 0\n",
    "    no_zeros = re.sub(\"[0-9]+\", \"0\", non_latin)\n",
    "    \n",
    "    # replace urls with <url>\n",
    "    result = re.sub(\"((http|ftp|https):\\/\\/)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\", \"<url>\", no_zeros)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_bpe_embs(text, model, sent_piece_processor):\n",
    "    preprocessed = preprocess_text(text)\n",
    "    subwords = sp.EncodeAsPieces(preprocessed)\n",
    "    filtered_subwords = list(filter(lambda x: x in model.vocab, subwords))\n",
    "    if len(filtered_subwords) > 0:\n",
    "        bpe_embs = model[filtered_subwords]\n",
    "    else:\n",
    "        bpe_embs = np.zeros((1, 300))\n",
    "    return bpe_embs\n",
    "\n",
    "def create_model_and_sent_piece(language):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(str(langs_to_bytepair_filepaths[lang]))\n",
    "    model = KeyedVectors.load_word2vec_format(langs_to_model_filepaths[lang], fvocab=langs_to_vocab_filepaths[lang], binary=True)\n",
    "    return model, sp\n",
    "    \n",
    "# test = \"ηὕρηκα Hey ążę\"\n",
    "# model, sp = create_model_and_sent_piece('english')\n",
    "# get_bpe_embs(test, model, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: english\n"
     ]
    }
   ],
   "source": [
    "# This gets the appropriate embedding for a particular language:\n",
    "# (Useful in the monolingual case)\n",
    "monolingual_index_to_embed_index = {}\n",
    "\n",
    "# Since there are loads of duplicates, to save on disk space,\n",
    "# these are different from the indices above. \n",
    "# Original database index 439 > embed index 15 > the embed itself\n",
    "# Original database index 440 > embed index 15 > the embed itself\n",
    "emb_indices = {}\n",
    "current_emb_index = -1\n",
    "\n",
    "target_word_embs = {}\n",
    "\n",
    "for lang in all_langs:\n",
    "    print(\"Processing: {}\".format(lang))\n",
    "    \n",
    "    lang_df = target.loc[target['lang'] == lang]\n",
    "\n",
    "    model, sp = create_model_and_sent_piece(lang)\n",
    "    \n",
    "    \n",
    "    last_emb = None\n",
    "    last_sent = None\n",
    "    for index, values in lang_df.iterrows():\n",
    "        target_word = values[1]\n",
    "        target_word_emb = get_bpe_embs(target_word, model, sp)\n",
    "        target_word_embs[index] = target_word_emb\n",
    "        \n",
    "        sent = values[0]\n",
    "\n",
    "        # This check just makes it quicker to process the many duplicate sentences.\n",
    "        # (which happen to be all in a row)\n",
    "        \n",
    "        if last_sent != sent:\n",
    "            last_sent = sent\n",
    "            # We pass model and sp in so we don't have to load them many times.\n",
    "            emb = get_bpe_embs(sent, model, sp)\n",
    "\n",
    "            last_emb = emb\n",
    "            \n",
    "            current_emb_index += 1\n",
    "            emb_indices[current_emb_index] = emb\n",
    "            monolingual_index_to_embed_index[index] = current_emb_index\n",
    "            \n",
    "        else:\n",
    "            emb = last_emb\n",
    "            monolingual_index_to_embed_index[index] = current_emb_index\n",
    "        \n",
    "# for orig_index, emb_index in monolingual_index_to_embed_index.items():\n",
    "#     print(orig_index, emb_index, target.loc[orig_index]['sentence'][:10])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train indices: 27299\n",
      "Num dev indices: 3328\n",
      "Num test indices: 4252\n",
      "\n",
      "Num train batches: 427\n",
      "Num dev batches: 52\n",
      "Num test batches: 67\n"
     ]
    }
   ],
   "source": [
    "# Adapted the keras model from here: https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.layers import Embedding\n",
    "import random\n",
    "\n",
    "# From here: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "target_lang = 'english'\n",
    "\n",
    "# 64 divides evenly, but should work with uneven batch_sizes\n",
    "batch_size = 64\n",
    "\n",
    "def get_all_batches(all_indices, batch_size):\n",
    "    random.shuffle(all_indices)\n",
    "\n",
    "    num_samples = len(all_indices)\n",
    "    \n",
    "    num_batches_excl_final = (num_samples) // batch_size\n",
    "    normal_batch_total = (num_batches_excl_final * batch_size)\n",
    "    \n",
    "    final_batch_size = num_samples - normal_batch_total\n",
    "    \n",
    "    batches = []\n",
    "    for i in range(num_batches_excl_final):\n",
    "        first_index = i*batch_size\n",
    "        last_index = (i+1)*batch_size\n",
    "        this_batch = all_indices[first_index:last_index]\n",
    "        batches.append(this_batch)\n",
    "    \n",
    "    \n",
    "    if final_batch_size != 0:\n",
    "        final_batch = all_indices[normal_batch_total:]\n",
    "        batches.append(final_batch)\n",
    "\n",
    "    return batches\n",
    "    \n",
    "all_indices = list(monolingual_index_to_embed_index.keys())\n",
    "all_train_indices = [x for x in all_indices if target.loc[x]['data_type'] == 'Train']\n",
    "all_dev_indices = [x for x in all_indices if target.loc[x]['data_type'] == 'Dev']\n",
    "all_test_indices = [x for x in all_indices if target.loc[x]['data_type'] == 'Test']\n",
    "print(\"Num train indices: {}\\nNum dev indices: {}\\nNum test indices: {}\\n\".format(len(all_train_indices),len(all_dev_indices),len(all_test_indices)))\n",
    "\n",
    "train_batches = get_all_batches(all_train_indices, batch_size)\n",
    "dev_batches = get_all_batches(all_dev_indices, batch_size)\n",
    "test_batches = get_all_batches(all_test_indices, batch_size)\n",
    "\n",
    "print(\"Num train batches: {}\\nNum dev batches: {}\\nNum test batches: {}\".format(len(train_batches),len(dev_batches),len(test_batches)))\n",
    "\n",
    "# print(batches[0])\n",
    "\n",
    "# print(emb_indices[monolingual_index_to_embed_index[34879]].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427\n"
     ]
    }
   ],
   "source": [
    "def get_gold_labels_from_batches(batches):\n",
    "    all_batch_labels = []\n",
    "    for batch in batches:\n",
    "        batch_labels = np.array([target.loc[x]['gold_label'] for x in batch])\n",
    "        all_batch_labels.append(batch_labels)\n",
    "    return all_batch_labels\n",
    "# Not sure which of these two to go with:\n",
    "\n",
    "train_y_batches = get_gold_labels_from_batches(train_batches)\n",
    "dev_y_batches = get_gold_labels_from_batches(dev_batches)\n",
    "test_y_batches = get_gold_labels_from_batches(test_batches)\n",
    "\n",
    "print(len(train_y_batches))\n",
    "    \n",
    "# train_ys = [target.loc[x]['gold_label'] for x in all_train_indices]\n",
    "# dev_ys = [target.loc[x]['gold_label'] for x in all_dev_indices]\n",
    "# test_ys = [target.loc[x]['gold_label'] for x in all_test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import inf\n",
    "\n",
    "def pad_sequence(emb, max_len):\n",
    "    \n",
    "    # TODO: Check if this is the right thing to do with inf vals?\n",
    "    emb[emb == -inf] = 0\n",
    "    emb[emb == inf] = 0\n",
    "    \n",
    "    # Pre-padding the vector with 0s\n",
    "    diff = max_len - emb.shape[0]\n",
    "    padded_emb = np.pad(emb, [(diff, 0), (0, 0)], mode='constant', constant_values=0)\n",
    "    \n",
    "    return padded_emb\n",
    "\n",
    "def get_padded_batches(unpadded_batches, emb_indices, target_index_to_embed_index):\n",
    "    # Should be 300 for all embeddings\n",
    "    emb_dim = emb_indices[0].shape[1]\n",
    "\n",
    "    padded_batches = []\n",
    "\n",
    "    progress = 0\n",
    "    for batch in unpadded_batches:\n",
    "        if progress % 50 == 0:\n",
    "            print(\"Batch: {}\".format(progress))\n",
    "\n",
    "        # Each batch might have a different longest sequence\n",
    "        longest_seq = 0\n",
    "\n",
    "        for mono_index in batch:\n",
    "            embed_index = monolingual_index_to_embed_index[mono_index]\n",
    "            emb_len = emb_indices[embed_index].shape[0] \n",
    "            if emb_len > longest_seq:\n",
    "                longest_seq = emb_len\n",
    "\n",
    "\n",
    "        batch_location = 0\n",
    "        # One emb_dim is for the sequence, and the other is for the target_word embedding\n",
    "        this_batch = np.zeros( (len(batch) , longest_seq , emb_dim + emb_dim) )\n",
    "\n",
    "        for mono_index in batch:\n",
    "            emb = emb_indices[monolingual_index_to_embed_index[mono_index]]\n",
    "            \n",
    "            target_word_emb = target_word_embs[mono_index]\n",
    "            seq_len = emb.shape[0]\n",
    "            repeated_target_word_emb = np.repeat(target_word_emb, seq_len, axis = 0)\n",
    "            \n",
    "            \n",
    "            padded_emb = pad_sequence(emb, longest_seq)\n",
    "            padded_target_word_emb = pad_sequence(repeated_target_word_emb, longest_seq)\n",
    "            \n",
    "            joined = np.concatenate([padded_emb, padded_target_word_emb], axis=1)\n",
    "\n",
    "            this_batch[batch_location] = joined\n",
    "\n",
    "\n",
    "            batch_location += 1\n",
    "\n",
    "        padded_batches.append(this_batch)\n",
    "        \n",
    "        progress += 1\n",
    "    \n",
    "    return padded_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting padded train batches...\n",
      "Batch: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[(-13, 0), (0, 0)] cannot contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-3b2cbcbfae42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Getting padded train batches...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpadded_train_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_padded_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonolingual_index_to_embed_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Getting padded dev batches...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpadded_dev_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_padded_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonolingual_index_to_embed_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Getting padded test batches...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-169-e4b71142b093>\u001b[0m in \u001b[0;36mget_padded_batches\u001b[1;34m(unpadded_batches, emb_indices, target_index_to_embed_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mpadded_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlongest_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mpadded_target_word_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeated_target_word_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlongest_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mjoined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpadded_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_target_word_emb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-169-e4b71142b093>\u001b[0m in \u001b[0;36mpad_sequence\u001b[1;34m(emb, max_len)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Pre-padding the vector with 0s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpadded_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'constant'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpadded_emb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m     \u001b[0mnarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m     \u001b[0mpad_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m     allowedkwargs = {\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36m_validate_lengths\u001b[1;34m(narray, number_elements)\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m             \u001b[0mfmt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%s cannot contain negative values.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1086\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfmt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumber_elements\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1087\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnormshp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [(-13, 0), (0, 0)] cannot contain negative values."
     ]
    }
   ],
   "source": [
    "print(\"Getting padded train batches...\")\n",
    "padded_train_batches = get_padded_batches(train_batches, emb_indices, monolingual_index_to_embed_index)\n",
    "print(\"Getting padded dev batches...\")\n",
    "padded_dev_batches = get_padded_batches(dev_batches, emb_indices, monolingual_index_to_embed_index)\n",
    "print(\"Getting padded test batches...\")\n",
    "padded_test_batches = get_padded_batches(test_batches, emb_indices, monolingual_index_to_embed_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.7037779, 0.390625]\n",
      "10 [0.68359673, 0.578125]\n",
      "20 [0.6997217, 0.5625]\n",
      "30 [0.6534382, 0.625]\n",
      "40 [0.6767038, 0.640625]\n",
      "50 [0.6645366, 0.625]\n",
      "60 [0.70213246, 0.609375]\n",
      "70 [0.6777805, 0.609375]\n",
      "80 [0.68530816, 0.515625]\n",
      "90 [0.6669874, 0.609375]\n",
      "100 [0.6885803, 0.546875]\n",
      "110 [0.6954412, 0.5625]\n",
      "120 [0.69717336, 0.53125]\n",
      "130 [0.7034153, 0.5]\n",
      "140 [0.7159567, 0.53125]\n",
      "150 [0.67704695, 0.578125]\n",
      "160 [0.71377164, 0.46875]\n",
      "170 [0.6777791, 0.578125]\n",
      "180 [0.67641383, 0.625]\n",
      "190 [0.6566706, 0.625]\n",
      "200 [0.6842252, 0.546875]\n",
      "210 [0.7009125, 0.546875]\n",
      "220 [0.70228577, 0.484375]\n",
      "230 [0.6682924, 0.59375]\n",
      "240 [0.6923089, 0.5625]\n",
      "250 [0.6458506, 0.640625]\n",
      "260 [0.71540207, 0.46875]\n",
      "270 [0.71391976, 0.546875]\n",
      "280 [0.6479999, 0.640625]\n",
      "290 [0.6670786, 0.59375]\n",
      "300 [0.6607239, 0.625]\n",
      "310 [0.7065612, 0.546875]\n",
      "320 [0.6749668, 0.625]\n",
      "330 [0.68700695, 0.5625]\n",
      "340 [0.64383495, 0.65625]\n",
      "350 [0.6707193, 0.578125]\n",
      "360 [0.6537538, 0.640625]\n",
      "370 [0.6885006, 0.546875]\n",
      "380 [0.71701723, 0.46875]\n",
      "390 [0.6775142, 0.578125]\n",
      "400 [0.66014266, 0.625]\n",
      "410 [0.7051296, 0.5]\n",
      "420 [0.72869366, 0.515625]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# def get_output_shape(input_shape):\n",
    "#     samples = input_shape[0]\n",
    "#     seq_len = input_shape[1]\n",
    "#     embedding_dim = input_shape[2]\n",
    "#     # Final shape needs to be same as embedding: (samples, sequence_length, embedding_dim)\n",
    "#     result_shape = (num_samples, seq_len, embedding_dim)\n",
    "#     return result_shape\n",
    "\n",
    "# def my_embedding_func():\n",
    "#     return None\n",
    "\n",
    "embed_dim = 300\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Lambda(my_embedding_func, output_shape = get_output_shape))\n",
    "# model.add(Embedding(max_features, output_dim=256))\n",
    "model.add(LSTM(128, input_shape=(None,embed_dim)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "# TODO: Make it generate different batches for different epochs\n",
    "# for epoch in epochs etc etc\n",
    "\n",
    "for batch_i in range(len(train_batches)):\n",
    "    \n",
    "    train_x_batch = padded_train_batches[batch_i]\n",
    "    train_y_batch = train_y_batches[batch_i]\n",
    "    \n",
    "    if batch_i % 10 == 0:\n",
    "        score = model.test_on_batch(train_x_batch, train_y_batch)\n",
    "        print(batch_i, score)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.train_on_batch(train_x_batch, train_y_batch)\n",
    "    \n",
    "    \n",
    "\n",
    "# model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "# score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "# For us, batch_size: not sure, timesteps = sent_len, data_dim=300\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(32, return_sequences=True,\n",
    "#                input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "# model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "# model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Generate dummy training data\n",
    "# x_train = np.random.random((1000, timesteps, data_dim))\n",
    "# y_train = np.random.random((1000, num_classes))\n",
    "\n",
    "# # Generate dummy validation data\n",
    "# x_val = np.random.random((100, timesteps, data_dim))\n",
    "# y_val = np.random.random((100, num_classes))\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=64, epochs=5,\n",
    "#           validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch, amount_to_pad):\n",
    "    padded_batch = np.pad(batch, [(0, 0),(amount_to_pad, 0), (0, 0)], mode='constant', constant_values=0)\n",
    "    return padded_batch\n",
    "\n",
    "def join_batches(list_of_batches):\n",
    "    record_seq = 0\n",
    "    for batch in list_of_batches:\n",
    "        seq_len = batch.shape[1]\n",
    "        if seq_len > record_seq:\n",
    "            record_seq = seq_len\n",
    "            \n",
    "    \n",
    "    list_of_padded_batches = []        \n",
    "    for batch in list_of_batches:\n",
    "        seq_len = batch.shape[1]\n",
    "        amount_to_pad = record_seq - seq_len\n",
    "        if amount_to_pad > 0:\n",
    "            padded_batch = pad_batch(batch, amount_to_pad)\n",
    "            \n",
    "        else:\n",
    "            padded_batch = batch\n",
    "            \n",
    "        list_of_padded_batches.append(padded_batch)\n",
    "          \n",
    "    joined_batches = np.concatenate(list_of_padded_batches, axis=0)\n",
    "\n",
    "    return joined_batches\n",
    "\n",
    "def join_y_batches(dev_y_batches):\n",
    "    joined_batches = np.concatenate(dev_y_batches, axis=0)\n",
    "    return joined_batches\n",
    "\n",
    "dev_xs = join_batches(padded_dev_batches)\n",
    "dev_ys = join_y_batches(dev_y_batches)\n",
    "\n",
    "test_xs = join_batches(padded_test_batches)\n",
    "test_ys = join_y_batches(test_y_batches)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3328/3328 [==============================] - 7s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6799369190747921, 0.5829326923076923]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: This is currently just using accuracy, would want to spit out predictions to use with the other evaluation system.\n",
    "\n",
    "model.evaluate(dev_xs, dev_ys, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4252/4252 [==============================] - 10s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6790529597736369, 0.5797271873181649]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: This is currently just using accuracy, would want to spit out predictions to use with the other evaluation system.\n",
    "\n",
    "model.evaluate(test_xs, test_ys, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
